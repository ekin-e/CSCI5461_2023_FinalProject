# -*- coding: utf-8 -*-
"""Support_Vector_Regressor.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1K-rDv11jY0xafMPUGlWQF7TNT1HeMzCs

### Training a Support Vector Regressor (SVR)
In this step, I am going to train a Support Vector regressor using the pipeline described on One_hot_encoding.ipynb. First, I am going to one hot encode the features in the same manner as before. I am going to proceed with the same example as before.
"""

#Importing the libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import ast

#Loading the dataframe from the previous notebook
df_i = pd.read_csv('CircadianRegions_2kb.csv',index_col=0)
df_i = df_i.sample(frac=0.1, replace=False, random_state=5461) # Sample a fraction of the df


# Create a new DataFrame with only certain columns
df = df_i.loc[:, ['gene', 'JTK_adjphase', 'region2kb']]
# Rename columns
df = df.rename(columns={'JTK_adjphase': 'Target', 
                        'gene': 'Genes',
                        'region2kb': '2kb'})

df['Features'] = df['2kb'].apply(lambda x: [x[i:i+8] for i in range(0, len(x)-7, 5)])
print(df)

"""Next, we one hot encode it using the same functions as the other notebook."""

# Functions from the other nothebook
def one_hot_encoding(features, all_features):
    one_hot = []
    for feature in all_features:
        if feature in features:
            one_hot.append(1)
        else:
            one_hot.append(0)
    return one_hot

def create_one_hot_encoding(df):

    '''
    This function takes a dataframe with a column named "Features" and creates a one hot encoding of the features.
    
    Parameters
    ----------
    df : pandas.DataFrame
    Dataframe with a column named "Features" that contains a list of features and a column named "Genes" that contains the gene names.
    Dataframe should also contain column called "Target" that contains the target.


        
    Returns
    ------- 
    df_one_hot : pandas.DataFrame
    Dataframe with one hot encoded features.
    
    '''
  
    #Preprocessing step

    # Making a list of all the features
    lists_of_features = df["Features"].values.flatten().tolist()

    # Nested list comprehension to create a single flattened list
    flattened_list = [item for sublist in lists_of_features for item in sublist]

    #making a set of the flattened lists features (removing duplicates)
    all_features = list(set(flattened_list))

    #  Creating one hot encoding dataframe keeping the same Gene names as index and the Target
    df_one_hot = pd.DataFrame(index=df.index)

    # Create columns names
    features_names = ["feature_" + str(i+1) for i in range(len(all_features))]

    # Add 100 columns with the names from the list to the DataFrame
    # for col in features_names:
    #     df_one_hot[col] = ""

    # Create a DataFrame with the same index as df_one_hot and columns as features_names
    new_cols = pd.DataFrame(index=df_one_hot.index, columns=features_names)

    # Concatenate the new DataFrame with df_one_hot
    df_one_hot = pd.concat([df_one_hot, new_cols], axis=1)

    # Adding one hot encoded features to the data frame
    for i,gene in enumerate(df.index):
        df_one_hot.loc[gene] = one_hot_encoding(df["Features"].values[i], all_features)

    #Adding the target from df
    df_one_hot.insert(0,"Target" , df["Target"])

    return df_one_hot

# Creating one hot encoding dataframe
lists_of_features = df["Features"].values.flatten().tolist()

# Nested list comprehension to create a single flattened list
flattened_list = [item for sublist in lists_of_features for item in sublist]

#making a set of the flattened lists features (removing duplicates)
all_features = list(set(flattened_list))

#  Creating one hot encoding dataframe keeping the same Gene names as index and the Target
df_one_hot = pd.DataFrame(index=df.index)
df["Features"].values[0]

df_one_hot = create_one_hot_encoding(df)
df_one_hot.insert(0, column='Genes', value=df['Genes'])

"""With the features and our target, we should be able to train an SVR. Before the training, I will make functions to perform cross-validation (CV) to evaluate our models. """

#Import CV Packages
from sklearn.model_selection import KFold

# RMSE function for two arrays
def rmse(y, y_pred):
    return np.sqrt(np.mean((y - y_pred)**2))

# Function that performs the cross validation
def run_cv(n_folds, model, X_train, y_train):
    """
    Args:
        n_folds (int) : how many folds of CV to do
        model (sklearn Model) : what model do we want to fit
        X_train (np.array) : feature matrix
        y_train (np.array) : target array
        
    Returns:
        a dictionary with scores from each fold for training and validation
            {'train' : [list of training scores],
             'val' : [list of validation scores]}
            - the length of each list = n_folds
    """
    
    folds = KFold(n_splits=n_folds).split(X_train, y_train)

    train_scores, val_scores = [], []
    for k, (train, val) in enumerate(folds):
        
        X_train_cv = X_train[train]
        y_train_cv = y_train[train]

        X_val_cv = X_train[val]
        y_val_cv = y_train[val]

        model.fit(X_train_cv, y_train_cv)

        y_train_cv_pred = model.predict(X_train_cv)
        y_val_cv_pred = model.predict(X_val_cv)

        train_acc = rmse(y_train_cv, y_train_cv_pred)
        val_acc = rmse(y_val_cv, y_val_cv_pred)

        train_scores.append(train_acc)
        val_scores.append(val_acc)

    print('%i Folds' % n_folds)
    print('Mean training rmse = %.3f +/- %.4f' % (np.mean(train_scores), np.std(train_scores)))
    print('Mean validation rmse = %.3f +/- %.4f' % (np.mean(val_scores), np.std(val_scores)))
    
    return {'train' : train_scores,
            'val' : val_scores}

"""Now we can split the dataset into train and test sets."""

#Importing Pachages
from sklearn.model_selection import train_test_split
from sklearn.svm import SVR

# Splitting the dataset into the Training set and Test set
X = df_one_hot.iloc[:, 2:].values
y = df_one_hot.iloc[:, 1].values
gene_names = df_one_hot.iloc[:, 0].values

# I will use 10% of the data for testing due to this being a small dataset
X_train, X_test, y_train, y_test, train_indices, test_indices = train_test_split(X, y, range(len(y)), test_size = 0.25, random_state = 7)

# Support Vector Regression Model
svr = SVR(C=1, kernel="rbf", verbose=3)

print("before training")
n_folds = 8
x = run_cv(n_folds, svr, X_train, y_train)
print("after training")

"""This is also not bad! More overfitted than the RF, but the **C hyperparameter** is a regularization parameter that can be tuned to avoid overfitting. In addition, the **kernel** can also be tuned (linear vs nonlinear)! This also about 15% RMSE for the validation but the standard deviation is large possibly due to a dataset these small (10 samples) with these many features (~70 features).

Next, we compute the feature importances using permutation importances.
"""

# Predicting the values for the test set
y_pred = svr.predict(X_test)

# Create a DataFrame containing the gene names and predicted values
predictions_df = pd.DataFrame({'Gene Name': gene_names[test_indices], 'Predicted Value': y_pred})

# Print the predictions DataFrame
print(predictions_df)

# Save the dataframe to a csv file
predictions_df.to_csv('predicted_values_svr.csv', index=False)

# #Importing package 
# from sklearn.inspection import permutation_importance

# # Splitting the training set into the Training validation set and Test validation sets
# #X_train_train, X_train_val, y_train_train, y_train_val = train_test_split(X_train, y_train, test_size=0.1)
# #svr.fit(X_train_train, y_train_train)

# r = permutation_importance(estimator=svr, X=X_train, y=y_train, n_repeats=3, random_state=7)

# """To make a more robust estimation of feature importance, **permutation importance** is used on a **subset** of the training set, as can be seen above! But for this examples it does not make much sense as we only have 10 samples."""

# # Getting the names of the features
# feature_names = df_one_hot.columns[1:]
# print(feature_names)

# # Getting the mean importances and sorted indexes   
# mean_importances = r.importances_mean
# sorted_idx = r.importances_mean.argsort()[::-1]

    
# # Plotting the feature importance
# plt.bar(feature_names[sorted_idx[:10]],mean_importances[sorted_idx[:10]])
# plt.xlabel('Features')
# plt.xticks(rotation=90)
# plt.ylabel('Importance')

# # create a dataframe with feature names and importance values
# feat_imp_df = pd.DataFrame({'feature_names': feature_names[sorted_idx], 
#                             'importance': mean_importances[sorted_idx]})

# # save the dataframe to a csv file
# feat_imp_df.to_csv('feature_importance_svr.csv', index=False)

# """Personally, I am not a huge fan of permutation importances because it tends to grade a lot of features as equally important. However, it is known as "model-agnostic", so it could be more robust because it does not rely on the inside of the model like the RF importances. It is good to try both. The most important feature was **Feature 41**, which is a close sequence to the one predicted by RF so this is interesting!"""

# # Determining the sequence attached to the most important feature

# sequence = all_features[sorted_idx[0]]

# print("The most important sequence predicted by random forest is: ", sequence)
# with open('sequence_svr.txt', 'w') as f:
#     f.write(sequence)