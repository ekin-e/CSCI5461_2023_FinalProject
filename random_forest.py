# -*- coding: utf-8 -*-
"""Copy of Random_Forest.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1v4c1FYpJlLJNFddXWuAtQ67_WMKSpAFU

### Training a Random Forest Regressor

In this step, I am going to train a random forest regressorm using the pipeline described on One_hot_encoding.ipynb. First, I am going to one hot encode the features in the same manner as before. I am going to proceed with the same example as before.
"""

#Importing the libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import ast

#Loading the dataframe from the previous notebook
df_i = pd.read_csv('CircadianRegions_2kb.csv',index_col=0)
df_i = df_i.sample(frac=0.1, replace=False, random_state=5461) # Sample a fraction of the df

# Create a new DataFrame with only certain columns
df = df_i.loc[:, ['gene', 'JTK_adjphase', 'region2kb']]
# Rename columns
df = df.rename(columns={'JTK_adjphase': 'Target', 
                        'gene': 'Genes',
                        'region2kb': '2kb'})

df['Features'] = df['2kb'].apply(lambda x: [x[i:i+8] for i in range(0, len(x)-7, 5)])
print(df)

"""Next, we one hot encode it using the same functions as the other notebook."""

# Functions from the other nothebook

def one_hot_encoding(features, all_features):
    one_hot = []
    for feature in all_features:
        if feature in features:
            one_hot.append(1)
        else:
            one_hot.append(0)
    return one_hot

def create_one_hot_encoding(df):

    '''
    This function takes a dataframe with a column named "Features" and creates a one hot encoding of the features.
    
    Parameters
    ----------
    df : pandas.DataFrame
    Dataframe with a column named "Features" that contains a list of features and a column named "Genes" that contains the gene names.
    Dataframe should also contain column called "Target" that contains the target.


        
    Returns
    ------- 
    df_one_hot : pandas.DataFrame
    Dataframe with one hot encoded features.
    
    '''
  
    #Preprocessing step

    # Making a list of all the features
    lists_of_features = df["Features"].values.flatten().tolist()

    # Nested list comprehension to create a single flattened list
    flattened_list = [item for sublist in lists_of_features for item in sublist]

    #making a set of the flattened lists features (removing duplicates)
    all_features = list(set(flattened_list))

    #  Creating one hot encoding dataframe keeping the same Gene names as index and the Target
    df_one_hot = pd.DataFrame(index=df.index)

    # Create columns names
    features_names = ["feature_" + str(i+1) for i in range(len(all_features))]

    # # Add 100 columns with the names from the list to the DataFrame
    # for col in features_names:
    #     df_one_hot[col] = ""

    # Create a DataFrame with the same index as df_one_hot and columns as features_names
    new_cols = pd.DataFrame(index=df_one_hot.index, columns=features_names)

    # Concatenate the new DataFrame with df_one_hot
    df_one_hot = pd.concat([df_one_hot, new_cols], axis=1)

    # Adding one hot encoded features to the data frame
    for i,gene in enumerate(df.index):
        df_one_hot.loc[gene] = one_hot_encoding(df["Features"].values[i], all_features)

    #Adding the target from df
    df_one_hot.insert(0,"Target" , df["Target"])

    return df_one_hot

# Creating one hot encoding dataframe
lists_of_features = df["Features"].values.flatten().tolist()

# Nested list comprehension to create a single flattened list
flattened_list = [item for sublist in lists_of_features for item in sublist]

#making a set of the flattened lists features (removing duplicates)
all_features = list(set(flattened_list))

#  Creating one hot encoding dataframe keeping the same Gene names as index and the Target
df_one_hot = pd.DataFrame(index=df.index)
df["Features"].values[0]

print("before one hot encoding")
df_one_hot = create_one_hot_encoding(df)
df_one_hot.insert(0, column='Genes', value=df['Genes'])
print("after one hot encoding")

"""With the features and our target, we should be able to train a random forest. Before the training, I will make functions to perform cross-validation (CV) to evaluate our models."""

#Import CV Packages
from sklearn.model_selection import KFold

# RMSE function for two arrays
def rmse(y, y_pred):
    return np.sqrt(np.mean((y - y_pred)**2))

# Function that performs the cross validation
def run_cv(n_folds, model, X_train, y_train):
    """
    Args:
        n_folds (int) : how many folds of CV to do
        model (sklearn Model) : what model do we want to fit
        X_train (np.array) : feature matrix
        y_train (np.array) : target array
        
    Returns:
        a dictionary with scores from each fold for training and validation
            {'train' : [list of training scores],
             'val' : [list of validation scores]}
            - the length of each list = n_folds
    """
    
    folds = KFold(n_splits=n_folds).split(X_train, y_train)

    train_scores, val_scores = [], []
    for k, (train, val) in enumerate(folds):
        
        X_train_cv = X_train[train]
        y_train_cv = y_train[train]

        X_val_cv = X_train[val]
        y_val_cv = y_train[val]

        model.fit(X_train_cv, y_train_cv)

        y_train_cv_pred = model.predict(X_train_cv)
        y_val_cv_pred = model.predict(X_val_cv)

        train_acc = rmse(y_train_cv, y_train_cv_pred)
        val_acc = rmse(y_val_cv, y_val_cv_pred)

        train_scores.append(train_acc)
        val_scores.append(val_acc)

    print('%i Folds' % n_folds)
    print('Mean training rmse = %.3f +/- %.4f' % (np.mean(train_scores), np.std(train_scores)))
    print('Mean validation rmse = %.3f +/- %.4f' % (np.mean(val_scores), np.std(val_scores)))
    
    return {'train' : train_scores,
            'val' : val_scores}

"""Now we can split the dataset into train and test sets"""

# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor

X = df_one_hot.iloc[:, 2:].values
y = df_one_hot.iloc[:, 1].values
gene_names = df_one_hot.iloc[:, 0].values

# I will use 25% of the data for testing due to this being a small dataset
X_train, X_test, y_train, y_test, train_indices, test_indices = train_test_split(X, y, range(len(y)), test_size = 0.25, random_state = 7)

# Random Forest Regressor with 1000 trees
Rf = RandomForestRegressor(n_estimators = 1000, random_state = 7, verbose=3, n_jobs=-1)

print("before training")
n_folds = 8
x = run_cv(n_folds, Rf, X_train, y_train)
print("after training")

# Predicting the values for the test set
y_pred = Rf.predict(X_test)

# Create a DataFrame containing the gene names and predicted values
predictions_df = pd.DataFrame({'Gene Name': gene_names[test_indices], 'Predicted Value': y_pred})

# Save the dataframe to a csv file
predictions_df.to_csv('predicted_values.csv', index=False)

# # Using the feature importance attribute of the random forest regressor to get the most important features

# # Getting the feature importance
# feature_importance = Rf.feature_importances_
# print(len(feature_importance))

# # Sorting the feature importance
# sorted_idx = np.argsort(feature_importance)[::-1]

# # Getting the names of the features
# feature_names = df_one_hot.columns[1:]

# # create a dataframe with feature names and importance values
# feat_imp_df = pd.DataFrame({'feature_names': feature_names[sorted_idx], 
#                             'importance': feature_importance[sorted_idx]})

# # save the dataframe to a csv file
# feat_imp_df.to_csv('feature_importance.csv', index=False)

# fig = plt.figure(dpi = 150)

# # Plotting the feature importance
# plt.bar(feature_names[sorted_idx[:10]],feature_importance[sorted_idx[:10]])
# plt.xlabel('Features')
# plt.xticks(rotation=90)
# plt.ylabel('Importance')

# # Save the figure as a PNG file
# fig.savefig('feature_importance.png', dpi=150)

# # Determining the sequence attached to the most important feature

# sequence = all_features[sorted_idx[0]]

# print("The most important sequence predicted by random forest is: ", sequence)
# with open('sequence.txt', 'w') as f:
#     f.write(sequence)